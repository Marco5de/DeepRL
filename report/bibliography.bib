% Encoding: UTF-8
@book{Sutton1998,
  added-at = {2019-07-13T10:11:53.000+0200},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  biburl = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
  edition = {Second},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  keywords = {},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  title = {Reinforcement Learning: An Introduction},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  year = {2018 }
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@article{Schulman2015TrustRP,
  title={Trust Region Policy Optimization},
  author={John Schulman and Sergey Levine and P. Abbeel and Michael I. Jordan and Philipp Moritz},
  journal={ArXiv},
  year={2015},
  volume={abs/1502.05477}
}

@article{schulman2017ppo,
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time. },
  added-at = {2019-12-16T18:31:56.000+0100},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  biburl = {https://www.bibsonomy.org/bibtex/24bbcce6aa1c42ae7f61ef8cf5475aa85/lanteunis},
  ee = {http://arxiv.org/abs/1707.06347},
  interhash = {f57ff463a90dbafb77d55a25aea8355c},
  intrahash = {4bbcce6aa1c42ae7f61ef8cf5475aa85},
  journal = {CoRR},
  keywords = {DRLAlgoComparison ppo reinforcement_learning},
  timestamp = {2019-12-18T21:15:59.000+0100},
  title = {Proximal Policy Optimization Algorithms.},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr1707.html#SchulmanWDRK17},
  volume = {abs/1707.06347},
  year = 2017
}

@inproceedings{Duan2016,
author = {Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
title = {Benchmarking Deep Reinforcement Learning for Continuous Control},
year = {2016},
publisher = {JMLR.org},
abstract = {Recently, researchers have made significant progress combining the advances in deep learning for learning feature representations with reinforcement learning. Some notable examples include training agents to play Atari games based on raw pixel data and to acquire advanced manipulation skills using raw sensory inputs. However, it has been difficult to quantify progress in the domain of continuous control due to the lack of a commonly adopted benchmark. In this work, we present a benchmark suite of continuous control tasks, including classic tasks like cart-pole swing-up, tasks with very high state and action dimensionality such as 3D humanoid locomotion, tasks with partial observations, and tasks with hierarchical structure. We report novel findings based on the systematic evaluation of a range of implemented reinforcement learning algorithms. Both the benchmark and reference implementations are released at https://github.com/rllab/rllab in order to facilitate experimental reproducibility and to encourage adoption by other researchers.},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {1329–1338},
numpages = {10},
location = {New York, NY, USA},
series = {ICML'16}
}

@inproceedings{Lillicrap2019,
  added-at = {2019-07-12T20:04:55.000+0200},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  biburl = {https://www.bibsonomy.org/bibtex/22708c349821330660afb992aec2be5d1/lanteunis},
  booktitle = {ICLR},
  crossref = {conf/iclr/2016},
  editor = {Bengio, Yoshua and LeCun, Yann},
  ee = {http://arxiv.org/abs/1509.02971},
  interhash = {b791167abe535c8525f6a9bf62fcc1ab},
  intrahash = {2708c349821330660afb992aec2be5d1},
  keywords = {},
  timestamp = {2019-07-12T20:04:55.000+0200},
  title = {Continuous control with deep reinforcement learning.},
  url = {http://dblp.uni-trier.de/db/conf/iclr/iclr2016.html#LillicrapHPHETS15},
  year = 2016
}

@article{Gottschalk2019,
author = {Gottschalk, Simon and Burger, Michael},
year = {2019},
month = {11},
pages = {},
title = {Differences and similarities between reinforcement learning and the classical optimal control framework},
volume = {19},
journal = {PAMM},
doi = {10.1002/pamm.201900390}
}

@misc{brockman2016openai,
  abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a
growing collection of benchmark problems that expose a common interface, and a
website where people can share their results and compare the performance of
algorithms. This whitepaper discusses the components of OpenAI Gym and the
design decisions that went into the software.},
  added-at = {2018-04-12T12:08:39.000+0200},
  author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  biburl = {https://www.bibsonomy.org/bibtex/2cdc8f927d6c8657ea82951a09e34161a/achakraborty},
  description = {[1606.01540] OpenAI Gym},
  interhash = {cfd0ba0b44eda9a3ca67480dfbf823a0},
  intrahash = {cdc8f927d6c8657ea82951a09e34161a},
  keywords = {2016 arxiv paper reinforcement-learning},
  note = {cite arxiv:1606.01540},
  timestamp = {2018-04-12T12:08:39.000+0200},
  title = {OpenAI Gym},
  url = {http://arxiv.org/abs/1606.01540},
  year = 2016
}

@misc{Moerland2020,
author = {Moerland, Thomas and Broekens, Joost and Jonker, Catholijn},
year = {2020},
month = {06},
pages = {},
title = {A Framework for Reinforcement Learning and Planning}
}

@inproceedings{Sutton1999,
author = {Sutton, Richard S. and McAllester, David and Singh, Satinder and Mansour, Yishay},
title = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
year = {1999},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and determining a policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, independent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters. Williams's REINFORCE method and actor-critic methods are examples of this approach. Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function. Using this result, we prove for the first time that a version of policy iteration with arbitrary differentiable function approximation is convergent to a locally optimal policy.},
booktitle = {Proceedings of the 12th International Conference on Neural Information Processing Systems},
pages = {1057–1063},
numpages = {7},
location = {Denver, CO},
series = {NIPS'99}
}

@incollection{NEURIPS2019_9015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@misc {benelot2018, author = {Benjamin Ellenberger}, title = {PyBullet Gymperium}, howpublished = {\url{ https://github.com/benelot/pybullet-gym}} , year = {2018--2019} }

@article{stable-baselines3,
  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},
  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {268},
  pages   = {1-8},
  url     = {http://jmlr.org/papers/v22/20-1364.html}
}

@article{Raffin2020,
  author    = {Antonin Raffin and
               Freek Stulp},
  title     = {Generalized State-Dependent Exploration for Deep Reinforcement Learning
               in Robotics},
  journal   = {CoRR},
  volume    = {abs/2005.05719},
  year      = {2020},
  url       = {https://arxiv.org/abs/2005.05719},
  eprinttype = {arXiv},
  eprint    = {2005.05719},
  timestamp = {Thu, 14 May 2020 16:56:02 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2005-05719.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

%@Comment{jabref-meta: databaseType:bibtex;}
